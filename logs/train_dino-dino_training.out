Loading cuda/12.3.0/gcc/11.3.0/zen2
  Loading requirement: gcc/11.3.0
============================================================
Starting job train_dino  (Job ID: 14516586)
Running on host: gpu-b10-1.zaratan.umd.edu
Cores per task: 4
GPUs allocated : 0
Job started at : Wed Oct 22 21:51:47 EDT 2025
Working directory: /scratch/zt1/project/heng-prj/user/ddewan/AID/TeamAID
============================================================

Wed Oct 22 21:51:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:41:00.0 Off |                    0 |
| N/A   25C    P0             54W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

1
/home/ddewan/miniconda3/envs/aid/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ddewan/miniconda3/envs/aid/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
0.1%0.3%0.4%0.6%0.7%0.8%1.0%1.1%1.3%1.4%1.5%1.7%1.8%1.9%2.1%2.2%2.4%2.5%2.6%2.8%2.9%3.1%3.2%3.3%3.5%3.6%3.8%3.9%4.0%4.2%4.3%4.4%4.6%4.7%4.9%5.0%5.1%5.3%5.4%5.6%5.7%5.8%6.0%6.1%6.3%6.4%6.5%6.7%6.8%6.9%7.1%7.2%7.4%7.5%7.6%7.8%7.9%8.1%8.2%8.3%8.5%8.6%8.8%8.9%9.0%9.2%9.3%9.4%9.6%9.7%9.9%10.0%10.1%10.3%10.4%10.6%10.7%10.8%11.0%11.1%11.3%11.4%11.5%11.7%11.8%11.9%12.1%12.2%12.4%12.5%12.6%12.8%12.9%13.1%13.2%13.3%13.5%13.6%13.8%13.9%14.0%14.2%14.3%14.4%14.6%14.7%14.9%15.0%15.1%15.3%15.4%15.6%15.7%15.8%16.0%16.1%16.3%16.4%16.5%16.7%16.8%16.9%17.1%17.2%17.4%17.5%17.6%17.8%17.9%18.1%18.2%18.3%18.5%18.6%18.8%18.9%19.0%19.2%19.3%19.4%19.6%19.7%19.9%20.0%20.1%20.3%20.4%20.6%20.7%20.8%21.0%21.1%21.3%21.4%21.5%21.7%21.8%22.0%22.1%22.2%22.4%22.5%22.6%22.8%22.9%23.1%23.2%23.3%23.5%23.6%23.8%23.9%24.0%24.2%24.3%24.5%24.6%24.7%24.9%25.0%25.1%25.3%25.4%25.6%25.7%25.8%26.0%26.1%26.3%26.4%26.5%26.7%26.8%27.0%27.1%27.2%27.4%27.5%27.6%27.8%27.9%28.1%28.2%28.3%28.5%28.6%28.8%28.9%29.0%29.2%29.3%29.5%29.6%29.7%29.9%30.0%30.1%30.3%30.4%30.6%30.7%30.8%31.0%31.1%31.3%31.4%31.5%31.7%31.8%32.0%32.1%32.2%32.4%32.5%32.6%32.8%32.9%33.1%33.2%33.3%33.5%33.6%33.8%33.9%34.0%34.2%34.3%34.5%34.6%34.7%34.9%35.0%35.1%35.3%35.4%35.6%35.7%35.8%36.0%36.1%36.3%36.4%36.5%36.7%36.8%37.0%37.1%37.2%37.4%37.5%37.6%37.8%37.9%38.1%38.2%38.3%38.5%38.6%38.8%38.9%39.0%39.2%39.3%39.5%39.6%39.7%39.9%40.0%40.1%40.3%40.4%40.6%40.7%40.8%41.0%41.1%41.3%41.4%41.5%41.7%41.8%42.0%42.1%42.2%42.4%42.5%42.7%42.8%42.9%43.1%43.2%43.3%43.5%43.6%43.8%43.9%44.0%44.2%44.3%44.5%44.6%44.7%44.9%45.0%45.2%45.3%45.4%45.6%45.7%45.8%46.0%46.1%46.3%46.4%46.5%46.7%46.8%47.0%47.1%47.2%47.4%47.5%47.7%47.8%47.9%48.1%48.2%48.3%48.5%48.6%48.8%48.9%49.0%49.2%49.3%49.5%49.6%49.7%49.9%50.0%50.2%50.3%50.4%50.6%50.7%50.8%51.0%51.1%51.3%51.4%51.5%51.7%51.8%52.0%52.1%52.2%52.4%52.5%52.7%52.8%52.9%53.1%53.2%53.3%53.5%53.6%53.8%53.9%54.0%54.2%54.3%54.5%54.6%54.7%54.9%55.0%55.2%55.3%55.4%55.6%55.7%55.8%56.0%56.1%56.3%56.4%56.5%56.7%56.8%57.0%57.1%57.2%57.4%57.5%57.7%57.8%57.9%58.1%58.2%58.3%58.5%58.6%58.8%58.9%59.0%59.2%59.3%59.5%59.6%59.7%59.9%60.0%60.2%60.3%60.4%60.6%60.7%60.9%61.0%61.1%61.3%61.4%61.5%61.7%61.8%62.0%62.1%62.2%62.4%62.5%62.7%62.8%62.9%63.1%63.2%63.4%63.5%63.6%63.8%63.9%64.0%64.2%64.3%64.5%64.6%64.7%64.9%65.0%65.2%65.3%65.4%65.6%65.7%65.9%66.0%66.1%66.3%66.4%66.5%66.7%66.8%67.0%67.1%67.2%67.4%67.5%67.7%67.8%67.9%68.1%68.2%68.4%68.5%68.6%68.8%68.9%69.0%69.2%69.3%69.5%69.6%69.7%69.9%70.0%70.2%70.3%70.4%70.6%70.7%70.9%71.0%71.1%71.3%71.4%71.5%71.7%71.8%72.0%72.1%72.2%72.4%72.5%72.7%72.8%72.9%73.1%73.2%73.4%73.5%73.6%73.8%73.9%74.0%74.2%74.3%74.5%74.6%74.7%74.9%75.0%75.2%75.3%75.4%75.6%75.7%75.9%76.0%76.1%76.3%76.4%76.5%76.7%76.8%77.0%77.1%77.2%77.4%77.5%77.7%77.8%77.9%78.1%78.2%78.4%78.5%78.6%78.8%78.9%79.0%79.2%79.3%79.5%79.6%79.7%79.9%80.0%80.2%80.3%80.4%80.6%80.7%80.9%81.0%81.1%81.3%81.4%81.6%81.7%81.8%82.0%82.1%82.2%82.4%82.5%82.7%82.8%82.9%83.1%83.2%83.4%83.5%83.6%83.8%83.9%84.1%84.2%84.3%84.5%84.6%84.7%84.9%85.0%85.2%85.3%85.4%85.6%85.7%85.9%86.0%86.1%86.3%86.4%86.6%86.7%86.8%87.0%87.1%87.2%87.4%87.5%87.7%87.8%87.9%88.1%88.2%88.4%88.5%88.6%88.8%88.9%89.1%89.2%89.3%89.5%89.6%89.7%89.9%90.0%90.2%90.3%90.4%90.6%90.7%90.9%91.0%91.1%91.3%91.4%91.6%91.7%91.8%92.0%92.1%92.2%92.4%92.5%92.7%92.8%92.9%93.1%93.2%93.4%93.5%93.6%93.8%93.9%94.1%94.2%94.3%94.5%94.6%94.7%94.9%95.0%95.2%95.3%95.4%95.6%95.7%95.9%96.0%96.1%96.3%96.4%96.6%96.7%96.8%97.0%97.1%97.2%97.4%97.5%97.7%97.8%97.9%98.1%98.2%98.4%98.5%98.6%98.8%98.9%99.1%99.2%99.3%99.5%99.6%99.7%99.9%100.0%
Train Dataset Size:  2638
Warning: Failed to fetch from internet; trying local repo...
Downloading: "https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth" to /home/ddewan/.cache/torch/hub/checkpoints/dino_resnet50_pretrain.pth
Using device: cuda
torch.cuda.is_available(): True
torch.cuda.device_count(): 1
torch.version.cuda: 12.8
Current CUDA device name: NVIDIA A100-SXM4-40GB
Epoch 1
-------------------------------
loss: 0.689489  [   64/ 2638]
loss: 0.182371  [ 2638/ 2638]
Epoch 2
-------------------------------
loss: 0.222095  [   64/ 2638]
loss: 0.113849  [ 2638/ 2638]
Epoch 3
-------------------------------
loss: 0.151300  [   64/ 2638]
loss: 0.100486  [ 2638/ 2638]
Epoch 4
-------------------------------
loss: 0.131979  [   64/ 2638]
loss: 0.266472  [ 2638/ 2638]
Epoch 5
-------------------------------
loss: 0.063456  [   64/ 2638]
loss: 0.333986  [ 2638/ 2638]
Epoch 6
-------------------------------
loss: 0.068636  [   64/ 2638]
loss: 0.008648  [ 2638/ 2638]
Epoch 7
-------------------------------
loss: 0.077586  [   64/ 2638]
loss: 0.197144  [ 2638/ 2638]
Epoch 8
-------------------------------
loss: 0.037715  [   64/ 2638]
loss: 0.018520  [ 2638/ 2638]
Epoch 9
-------------------------------
loss: 0.041087  [   64/ 2638]
loss: 0.110946  [ 2638/ 2638]
Epoch 10
-------------------------------
loss: 0.066091  [   64/ 2638]
loss: 0.024669  [ 2638/ 2638]
Epoch 11
-------------------------------
loss: 0.123488  [   64/ 2638]
loss: 0.002326  [ 2638/ 2638]
Epoch 12
-------------------------------
loss: 0.041078  [   64/ 2638]
loss: 0.349325  [ 2638/ 2638]
Epoch 13
-------------------------------
loss: 0.031432  [   64/ 2638]
loss: 0.137774  [ 2638/ 2638]
Epoch 14
-------------------------------
loss: 0.045142  [   64/ 2638]
loss: 0.005301  [ 2638/ 2638]
Epoch 15
-------------------------------
loss: 0.012664  [   64/ 2638]
loss: 0.348922  [ 2638/ 2638]
Epoch 16
-------------------------------
loss: 0.042039  [   64/ 2638]
loss: 0.002348  [ 2638/ 2638]
Epoch 17
-------------------------------
loss: 0.022492  [   64/ 2638]
loss: 0.537777  [ 2638/ 2638]
Epoch 18
-------------------------------
loss: 0.005093  [   64/ 2638]
loss: 0.211495  [ 2638/ 2638]
Epoch 19
-------------------------------
loss: 0.044529  [   64/ 2638]
loss: 0.039381  [ 2638/ 2638]
Epoch 20
-------------------------------
loss: 0.051421  [   64/ 2638]
loss: 0.000642  [ 2638/ 2638]
Epoch 21
-------------------------------
loss: 0.048343  [   64/ 2638]
loss: 0.044323  [ 2638/ 2638]
Epoch 22
-------------------------------
loss: 0.003011  [   64/ 2638]
loss: 0.056619  [ 2638/ 2638]
Epoch 23
-------------------------------
loss: 0.002310  [   64/ 2638]
loss: 0.048535  [ 2638/ 2638]
Epoch 24
-------------------------------
loss: 0.003612  [   64/ 2638]
loss: 0.455229  [ 2638/ 2638]
Epoch 25
-------------------------------
loss: 0.004994  [   64/ 2638]
loss: 0.056728  [ 2638/ 2638]
Epoch 26
-------------------------------
loss: 0.029542  [   64/ 2638]
loss: 0.276639  [ 2638/ 2638]
Epoch 27
-------------------------------
loss: 0.004969  [   64/ 2638]
loss: 0.014952  [ 2638/ 2638]
Epoch 28
-------------------------------
loss: 0.007827  [   64/ 2638]
loss: 0.015658  [ 2638/ 2638]
Epoch 29
-------------------------------
loss: 0.011427  [   64/ 2638]
loss: 0.202125  [ 2638/ 2638]
Epoch 30
-------------------------------
loss: 0.006713  [   64/ 2638]
loss: 0.006839  [ 2638/ 2638]
Epoch 31
-------------------------------
loss: 0.007444  [   64/ 2638]
loss: 0.093276  [ 2638/ 2638]
Epoch 32
-------------------------------
loss: 0.002409  [   64/ 2638]
loss: 0.003476  [ 2638/ 2638]
Epoch 33
-------------------------------
loss: 0.002764  [   64/ 2638]
loss: 0.053397  [ 2638/ 2638]
Epoch 34
-------------------------------
loss: 0.000329  [   64/ 2638]
loss: 0.004249  [ 2638/ 2638]
Epoch 35
-------------------------------
loss: 0.006212  [   64/ 2638]
loss: 0.004788  [ 2638/ 2638]
Epoch 36
-------------------------------
loss: 0.007426  [   64/ 2638]
loss: 0.005924  [ 2638/ 2638]
Epoch 37
-------------------------------
loss: 0.009853  [   64/ 2638]
loss: 0.000814  [ 2638/ 2638]
Epoch 38
-------------------------------
loss: 0.006432  [   64/ 2638]
loss: 0.000117  [ 2638/ 2638]
Epoch 39
-------------------------------
loss: 0.001301  [   64/ 2638]
loss: 0.012881  [ 2638/ 2638]
Epoch 40
-------------------------------
loss: 0.002380  [   64/ 2638]
loss: 0.001950  [ 2638/ 2638]
Epoch 41
-------------------------------
loss: 0.007889  [   64/ 2638]
loss: 0.004263  [ 2638/ 2638]
Epoch 42
-------------------------------
loss: 0.001671  [   64/ 2638]
loss: 0.632136  [ 2638/ 2638]
Epoch 43
-------------------------------
loss: 0.016718  [   64/ 2638]
loss: 0.214428  [ 2638/ 2638]
Epoch 44
-------------------------------
loss: 0.105265  [   64/ 2638]
loss: 0.286960  [ 2638/ 2638]
Epoch 45
-------------------------------
loss: 0.011420  [   64/ 2638]
loss: 0.000896  [ 2638/ 2638]
Epoch 46
-------------------------------
loss: 0.006474  [   64/ 2638]
loss: 0.021278  [ 2638/ 2638]
Epoch 47
-------------------------------
loss: 0.000710  [   64/ 2638]
loss: 0.001959  [ 2638/ 2638]
Epoch 48
-------------------------------
loss: 0.003918  [   64/ 2638]
loss: 0.072073  [ 2638/ 2638]
Epoch 49
-------------------------------
loss: 0.002338  [   64/ 2638]
loss: 0.000541  [ 2638/ 2638]
Epoch 50
-------------------------------
loss: 0.001562  [   64/ 2638]
loss: 0.131021  [ 2638/ 2638]
Training Complete.

Job finished at: Wed Oct 22 21:58:38 EDT 2025
Exit code: 
============================================================
